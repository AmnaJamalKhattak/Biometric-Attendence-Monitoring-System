{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IGMPlhbREs1u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create a Kaggle directory in your Colab environment\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# Move your kaggle.json file to the .kaggle folder\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "# Set permissions for the file\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAVTtVnwFKAX",
        "outputId": "3213a3a8-018b-4937-c2b2-ad7a91717f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXaxcw39Fite",
        "outputId": "21d3ad91-fed8-47e8-b94a-39d203ffde28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the latest version of the dataset\n",
        "path = kagglehub.dataset_download(\"arifmia/football-player-image-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG56Y7opFnTA",
        "outputId": "460a8f1c-cf05-4540-edca-1cb6a189a09b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Files:\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1 contains 0 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player contains 0 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player/messi contains 121 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player/ronalu contains 133 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player/devala contains 131 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player/dimaria contains 130 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player/neimar contains 127 files\n",
            "/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player/matiaz contains 124 files\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the dataset directory\n",
        "print(\"Dataset Files:\")\n",
        "for root, dirs, files in os.walk(path):\n",
        "    print(root, \"contains\", len(files), \"files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5tGx6cpoFr9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll player\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yB4fjhpkGKJ-",
        "outputId": "c4e43994-1e25-498b-92a1-628c143c70ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Collecting Pillow<10.3.0,>=10.2.0 (from facenet-pytorch)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.32.3)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, Pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, facenet-pytorch\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.2.0 facenet-pytorch-2.6.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "944698feeb24448dadfbf71fa221540b",
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install facenet-pytorch\n",
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WTs6H73-GRAi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sbDwYGWQGg9P"
      },
      "outputs": [],
      "source": [
        "# Updated dataset path\n",
        "dataset_path = '/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll player'\n",
        "output_path = '/content/processed_images'\n",
        "\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "56eda4820092482089ed408bcf58224b",
            "d33a9467f6d7406e9ed23c092cf6ba9d",
            "d692f541fe624c9a956251c370a9ef05",
            "605f4acc75494a029ad3403740b2b4c7",
            "9b58b41631c44f2a98b76d72b3804600",
            "0edec934e7cc4c4a91477e434f7395ab",
            "d8279d07e6964ee48d497b1d60649963",
            "8020f5630e7d48d48b9f5066b5b267a8",
            "bd51a6eaa3304ffd9045fba69436a2a6",
            "88f00e4a604f40318dd8913235a3088f",
            "3c5274f98c304612b2fbfb0f5fab49ff"
          ]
        },
        "id": "7LmLWPdeGji1",
        "outputId": "2c059a30-b477-45f0-d2e4-b47c22631347"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56eda4820092482089ed408bcf58224b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize models\n",
        "mtcnn = MTCNN(keep_all=True, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "def process_images_and_generate_embeddings(dataset_path, output_path):\n",
        "    player_embeddings = {}\n",
        "\n",
        "    # Iterate through player folders\n",
        "    for player_folder in os.listdir(dataset_path):\n",
        "        player_path = os.path.join(dataset_path, player_folder)\n",
        "\n",
        "        if os.path.isdir(player_path):\n",
        "            # Create output folder for each player\n",
        "            player_output = os.path.join(output_path, player_folder)\n",
        "            os.makedirs(player_output, exist_ok=True)\n",
        "\n",
        "            player_faces = []\n",
        "\n",
        "            # Process each image in the player's folder\n",
        "            for image_file in os.listdir(player_path):\n",
        "                try:\n",
        "                    # Full path to the image\n",
        "                    img_path = os.path.join(player_path, image_file)\n",
        "\n",
        "                    # Open image\n",
        "                    img = Image.open(img_path)\n",
        "\n",
        "                    # Detect faces\n",
        "                    faces, probs = mtcnn(img, return_prob=True)\n",
        "\n",
        "                    if faces is not None:\n",
        "                        # Generate embeddings\n",
        "                        with torch.no_grad():\n",
        "                            embeddings = resnet(faces)\n",
        "\n",
        "                        # Store embeddings\n",
        "                        for embedding in embeddings:\n",
        "                            player_faces.append(embedding.numpy())\n",
        "\n",
        "                        # Save detected faces\n",
        "                        for i, face in enumerate(faces):\n",
        "                            face_img = transforms.ToPILImage()(face)\n",
        "                            face_img.save(os.path.join(player_output, f'{image_file}_face_{i}.jpg'))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {image_file} for {player_folder}: {e}\")\n",
        "\n",
        "            # Average embeddings for the player\n",
        "            if player_faces:\n",
        "                player_embeddings[player_folder] = np.mean(player_faces, axis=0)\n",
        "\n",
        "    return player_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXewFALuK1NU",
        "outputId": "b9597846-00b8-495f-ff8e-3d45120559b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "['footbll  player']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Print the full path details\n",
        "print(os.path.exists('/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1'))\n",
        "\n",
        "# List the contents of the directory\n",
        "print(os.listdir('/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZRSO6bpLFEi",
        "outputId": "f21d11f4-8f14-4c77-88d1-dbfd0352cdb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Players found: ['messi', 'ronalu', 'devala', 'dimaria', 'neimar', 'matiaz']\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 7 faces for messi\n",
            "Detected 3 faces for messi\n",
            "Detected 3 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 126 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 3 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 4 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Skipping non-image file: lionel-messi-barcelona-lionel-messi-futbolist-poza-sportsmen.webp\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 7 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 4 faces for messi\n",
            "Error processing image42.png for messi: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 2, 39, 113] to have 3 channels, but got 2 channels instead\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 4 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 15 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 2 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Detected 1 faces for messi\n",
            "Processed messi: 199 face embeddings\n",
            "Detected 1 faces for ronalu\n",
            "Detected 5 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 4 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Skipping non-image file: vector12.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: _390575742906.app.webp\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 5 faces for ronalu\n",
            "Skipping non-image file: vector6.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Skipping non-image file: vector13.svg\n",
            "Skipping non-image file: vector7.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Error processing 6684a913a31095c551b478a5.png for ronalu: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 334, 536] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 7 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Error processing image43.jpg for ronalu: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Skipping non-image file: vector11.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 27 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: cristiano-ronaldo.webp\n",
            "Detected 1 faces for ronalu\n",
            "Error processing 9348_image_5-1920x1080.png for ronalu: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 3 faces for ronalu\n",
            "Skipping non-image file: vector10.svg\n",
            "Detected 2 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 5 faces for ronalu\n",
            "Detected 4 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Skipping non-image file: vector5.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Error processing cristiano-ronaldo-8002334_1280.png for ronalu: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 12 faces for ronalu\n",
            "Detected 3 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Skipping non-image file: 12edab40-71c0-11ef-a237-49738a978907.jpg.webp\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Error processing image42.png for ronalu: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 2, 39, 113] to have 3 channels, but got 2 channels instead\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: _330603286208.app.webp\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: vector4.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: vector9.svg\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: vector3.svg\n",
            "Skipping non-image file: newindianexpress2F2024-062F7e3f7d82-006b-4262-8521-af08bc1d43342FRonaldo.webp\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: vector8.svg\n",
            "Skipping non-image file: vector.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 2 faces for ronalu\n",
            "Skipping non-image file: IMG_0881.webp\n",
            "Detected 1 faces for ronalu\n",
            "Skipping non-image file: vector2.svg\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Detected 1 faces for ronalu\n",
            "Processed ronalu: 130 face embeddings\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector12.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector6.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector13.svg\n",
            "Detected 2 faces for devala\n",
            "Skipping non-image file: vector7.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Skipping non-image file: BOMBAZO-Dybala-puede-irse-de-la-Roma.jpg.webp\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: roma-paulo-dybala-ansa.webp\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Skipping non-image file: vector11.svg\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: Dybala.jpg.webp\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 4 faces for devala\n",
            "Error processing paulo-dybala-85.png for devala: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: a-celebrates-after-scoring-the-teams-third-goal-during-the-FIFA-World-Cup-2.webp\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector10.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Error processing image41.png for devala: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 2, 39, 113] to have 3 channels, but got 2 channels instead\n",
            "Detected 4 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 3 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 3 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector5.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Error processing image42.jpg for devala: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Skipping non-image file: wciumfhmh4di4kmogghw.webp\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Error processing 70e495e41938-dybala.png for devala: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 433, 433] to have 3 channels, but got 4 channels instead\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 3 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Error processing 24527.vresize.350.350.medium.55.png for devala: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 211, 211] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector4.svg\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector9.svg\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector3.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: wuheicuzahoymiqg8fbn.webp\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector8.svg\n",
            "Skipping non-image file: vector.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 3 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Skipping non-image file: vector2.svg\n",
            "Detected 1 faces for devala\n",
            "Detected 2 faces for devala\n",
            "Detected 1 faces for devala\n",
            "Processed devala: 110 face embeddings\n",
            "Detected 3 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 3 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 17 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Skipping non-image file: CgAGVWOgxkqAE60QAACVUUREjA4842.jpg.webp\n",
            "Skipping non-image file: vector12.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector6.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector13.svg\n",
            "Skipping non-image file: vector7.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 4 faces for dimaria\n",
            "Error processing image40.png for dimaria: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 2, 39, 113] to have 3 channels, but got 2 channels instead\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector11.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Skipping non-image file: vector14.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 3 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Error processing creatividad-del-fichaje-de-angel-di-maria-por-el-benfica--slbenfica.png for dimaria: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 282, 331] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 3 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector10.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Error processing 871.vresize.350.350.medium.27.png for dimaria: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Skipping non-image file: vector5.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 12 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Error processing di-maria-angel-di-maria.gif for dimaria: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 4 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Error processing GOAL20-20Blank20WEB20-20Facebook20-202024-06-09T203956.033.png for dimaria: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 649, 1153] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 5 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 6 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector4.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 3 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector9.svg\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector3.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector8.svg\n",
            "Skipping non-image file: vector.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 3 faces for dimaria\n",
            "Skipping non-image file: _330783779915.app.webp\n",
            "Detected 2 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Detected 1 faces for dimaria\n",
            "Skipping non-image file: vector2.svg\n",
            "Detected 1 faces for dimaria\n",
            "Detected 2 faces for dimaria\n",
            "Processed dimaria: 131 face embeddings\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector12.svg\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Skipping non-image file: vector6.svg\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Skipping non-image file: vector13.svg\n",
            "Skipping non-image file: vector7.svg\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector11.svg\n",
            "Detected 5 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Error processing image38.png for neimar: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 2, 39, 113] to have 3 channels, but got 2 channels instead\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 3 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 3 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector10.svg\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Skipping non-image file: vector5.svg\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 3 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: BUS_20240914_BUS_234839_Neymar.webp\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 4 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector4.svg\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector9.svg\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector3.svg\n",
            "Detected 3 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Error processing 603_neymar.jpg for neimar: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Error processing 713.vresize.350.350.medium.34.png for neimar: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 211, 211] to have 3 channels, but got 4 channels instead\n",
            "Skipping non-image file: vector8.svg\n",
            "Skipping non-image file: vector.svg\n",
            "Error processing hero_22-23-neymar.png for neimar: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Skipping non-image file: soccer-player-brazil-neymar.webp\n",
            "Detected 2 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 2 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Skipping non-image file: vector2.svg\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Detected 1 faces for neimar\n",
            "Processed neimar: 107 face embeddings\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Error processing image36.png for matiaz: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 2, 39, 113] to have 3 channels, but got 2 channels instead\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Skipping non-image file: vector12.svg\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Skipping non-image file: vector6.svg\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector13.svg\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector7.svg\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: _128044617_emimartinez.jpg.webp\n",
            "Detected 2 faces for matiaz\n",
            "Error processing 12258.vresize.350.350.medium.96.png for matiaz: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector11.svg\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 3 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector10.svg\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 3 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Error processing GOAL20-20Blank20WEB20-20Facebook20-202024-10-15T142000.188.png for matiaz: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 649, 1153] to have 3 channels, but got 4 channels instead\n",
            "Error processing Emiliano20Martinez20Lisandro20Martinez20Argentina20Copa20America202024.png for matiaz: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 257, 457] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector5.svg\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Error processing EmilianoMartinez2-_PhotoPoster.png for matiaz: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 512, 385] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 4 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 4 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector4.svg\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector9.svg\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Skipping non-image file: vector3.svg\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 3 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Skipping non-image file: vector8.svg\n",
            "Skipping non-image file: vector.svg\n",
            "Detected 1 faces for matiaz\n",
            "Error processing GOAL20-20Blank20WEB20-20Facebook20-202024-06-21T090934.415.png for matiaz: Given groups=1, weight of size [10, 3, 3, 3], expected input[1, 4, 649, 1153] to have 3 channels, but got 4 channels instead\n",
            "Detected 1 faces for matiaz\n",
            "Detected 3 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Skipping non-image file: vector2.svg\n",
            "Detected 2 faces for matiaz\n",
            "Detected 2 faces for matiaz\n",
            "Detected 1 faces for matiaz\n",
            "Processed matiaz: 120 face embeddings\n",
            "\n",
            "Embeddings saved to /content/player_embeddings.pkl\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Configuration\n",
        "base_path = '/root/.cache/kagglehub/datasets/arifmia/football-player-image-dataset/versions/1/footbll  player'\n",
        "output_path = '/content/processed_images'\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Advanced MTCNN Configuration\n",
        "mtcnn = MTCNN(\n",
        "    image_size=160,       # Consistent image size\n",
        "    margin=0,             # No margin around detected face\n",
        "    min_face_size=20,     # Minimum face size to detect\n",
        "    thresholds=[0.6, 0.7, 0.7],  # Relaxed detection thresholds\n",
        "    factor=0.709,         # Scale factor for multi-scale detection\n",
        "    keep_all=True,        # Keep all detected faces\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "# Initialize face recognition model\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Advanced image preprocessing for consistent input\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((160, 160)),  # Consistent resize\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
        "    ])\n",
        "    return transform(img)\n",
        "\n",
        "def calculate_face_similarity(known_embedding, face_embedding):\n",
        "    \"\"\"Calculate cosine similarity between face embeddings\"\"\"\n",
        "    cosine_similarity = 1 - torch.nn.functional.cosine_similarity(\n",
        "        torch.tensor(known_embedding).unsqueeze(0),\n",
        "        torch.tensor(face_embedding).unsqueeze(0)\n",
        "    ).item()\n",
        "    return cosine_similarity\n",
        "\n",
        "def adaptive_threshold(embeddings):\n",
        "    \"\"\"Dynamically calculate recognition threshold\"\"\"\n",
        "    distances = [np.linalg.norm(emb) for emb in embeddings]\n",
        "    mean_distance = np.mean(distances)\n",
        "    std_distance = np.std(distances)\n",
        "    return mean_distance + 0.5 * std_distance\n",
        "\n",
        "def process_images_and_generate_embeddings(base_path, output_path):\n",
        "    player_embeddings = {}\n",
        "\n",
        "    # List all player folders\n",
        "    player_folders = os.listdir(base_path)\n",
        "\n",
        "    print(\"Players found:\", player_folders)\n",
        "\n",
        "    for player_folder in player_folders:\n",
        "        player_path = os.path.join(base_path, player_folder)\n",
        "\n",
        "        if os.path.isdir(player_path):\n",
        "            # Create output folder for each player\n",
        "            player_output = os.path.join(output_path, player_folder)\n",
        "            os.makedirs(player_output, exist_ok=True)\n",
        "\n",
        "            player_faces = []\n",
        "\n",
        "            # Process each image in the player's folder\n",
        "            for image_file in os.listdir(player_path):\n",
        "                try:\n",
        "                    # Full path to the image\n",
        "                    img_path = os.path.join(player_path, image_file)\n",
        "\n",
        "                    # Check if it's an image file\n",
        "                    if not image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                        print(f\"Skipping non-image file: {image_file}\")\n",
        "                        continue\n",
        "\n",
        "                    # Open image\n",
        "                    img = Image.open(img_path)\n",
        "\n",
        "                    # Detect faces\n",
        "                    faces, probs = mtcnn(img, return_prob=True)\n",
        "\n",
        "                    if faces is not None and len(faces) > 0:\n",
        "                        print(f\"Detected {len(faces)} faces for {player_folder}\")\n",
        "\n",
        "                        # Generate embeddings with high confidence faces\n",
        "                        high_conf_faces = [face for face, prob in zip(faces, probs) if prob > 0.95]\n",
        "\n",
        "                        if high_conf_faces:\n",
        "                            with torch.no_grad():\n",
        "                                # Stack the faces into a batch tensor\n",
        "                                face_tensor = torch.stack(high_conf_faces)\n",
        "\n",
        "                                # Get embeddings for the batch of faces\n",
        "                                embeddings = resnet(face_tensor)\n",
        "\n",
        "                            # Store embeddings for the player\n",
        "                            for embedding in embeddings:\n",
        "                                player_faces.append(embedding.numpy())\n",
        "\n",
        "                        # Save detected faces\n",
        "                        for i, face in enumerate(high_conf_faces):\n",
        "                            face_img = transforms.ToPILImage()(face)\n",
        "                            face_img.save(os.path.join(player_output, f'{image_file}_face_{i}.jpg'))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {image_file} for {player_folder}: {e}\")\n",
        "\n",
        "            # Average embeddings for the player\n",
        "            if player_faces:\n",
        "                player_embeddings[player_folder] = np.mean(player_faces, axis=0)\n",
        "                print(f\"Processed {player_folder}: {len(player_faces)} face embeddings\")\n",
        "            else:\n",
        "                print(f\"No faces detected for {player_folder}\")\n",
        "\n",
        "    return player_embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "player_embeddings = process_images_and_generate_embeddings(base_path, output_path)\n",
        "\n",
        "# Save embeddings for later use\n",
        "import pickle\n",
        "with open('/content/player_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(player_embeddings, f)\n",
        "\n",
        "print(\"\\nEmbeddings saved to /content/player_embeddings.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0nBccWWLRE-",
        "outputId": "9f464aa2-d30c-4d19-caf4-99d4bdcaf03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attendance Tracking System is Ready!\n",
            "Players in System: ['messi', 'ronalu', 'devala', 'dimaria', 'neimar', 'matiaz']\n",
            "\n",
            "To mark attendance:\n",
            "1. Prepare a test image with players\n",
            "2. Use attendance_workflow(image_path)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torchvision.transforms as transforms\n",
        "import pickle\n",
        "\n",
        "# Load pre-generated embeddings\n",
        "with open('/content/player_embeddings.pkl', 'rb') as f:\n",
        "    player_embeddings = pickle.load(f)\n",
        "\n",
        "# Advanced MTCNN Configuration\n",
        "mtcnn = MTCNN(\n",
        "    image_size=160,       # Consistent image size\n",
        "    margin=0,             # No margin around detected face\n",
        "    min_face_size=20,     # Minimum face size to detect\n",
        "    thresholds=[0.6, 0.7, 0.7],  # Relaxed detection thresholds\n",
        "    factor=0.709,         # Scale factor for multi-scale detection\n",
        "    keep_all=True,        # Keep all detected faces\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "# Initialize face recognition model\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "def calculate_face_similarity(known_embedding, face_embedding):\n",
        "    \"\"\"Calculate cosine similarity between face embeddings\"\"\"\n",
        "    cosine_similarity = 1 - torch.nn.functional.cosine_similarity(\n",
        "        torch.tensor(known_embedding).unsqueeze(0),\n",
        "        torch.tensor(face_embedding).unsqueeze(0)\n",
        "    ).item()\n",
        "    return cosine_similarity\n",
        "\n",
        "def mark_attendance(input_image, player_embeddings, threshold=0.6):\n",
        "    \"\"\"Mark attendance with clear player name display\"\"\"\n",
        "    # Load input image\n",
        "    img = Image.open(input_image)\n",
        "\n",
        "    # Detect faces\n",
        "    faces, probs = mtcnn(img, return_prob=True)\n",
        "\n",
        "    # Initialize attendance results with all players marked absent\n",
        "    attendance_results = {player: False for player in player_embeddings.keys()}\n",
        "\n",
        "    if faces is not None and len(faces) > 0:\n",
        "        # Generate embeddings for detected faces\n",
        "        with torch.no_grad():\n",
        "            input_embeddings = resnet(faces)\n",
        "\n",
        "        # Analyze each detected face\n",
        "        for i, (face, prob) in enumerate(zip(faces, probs)):\n",
        "            # Compute similarities to all known players\n",
        "            similarities = {}\n",
        "            for player, known_embedding in player_embeddings.items():\n",
        "                similarity = calculate_face_similarity(known_embedding, input_embeddings[i])\n",
        "                similarities[player] = similarity\n",
        "\n",
        "            # Find best match\n",
        "            best_match = min(similarities, key=similarities.get)\n",
        "            best_similarity = similarities[best_match]\n",
        "\n",
        "            # Mark attendance if similarity is below threshold\n",
        "            if best_similarity < threshold:\n",
        "                attendance_results[best_match] = True\n",
        "\n",
        "    # Print clear attendance results\n",
        "    print(\"\\nAttendance Results:\")\n",
        "    for player in sorted(attendance_results.keys()):\n",
        "        status = \"Present ✓\" if attendance_results[player] else \"Absent ✗\"\n",
        "        print(f\"{player}: {status}\")\n",
        "\n",
        "    return attendance_results\n",
        "\n",
        "def update_attendance_log(attendance_results, log_path='/content/attendance_log.csv'):\n",
        "    \"\"\"Update attendance log with improved error handling\"\"\"\n",
        "    try:\n",
        "        # Get current date and time\n",
        "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        # Convert boolean attendance to 'Present'/'Absent'\n",
        "        attendance_results = {player: \"Present\" if present else \"Absent\" for player, present in attendance_results.items()}\n",
        "\n",
        "        # Prepare attendance record\n",
        "        record = pd.DataFrame({\n",
        "            'Timestamp': [current_time],\n",
        "            **attendance_results\n",
        "        })\n",
        "\n",
        "        # Check if log exists\n",
        "        if os.path.exists(log_path):\n",
        "            # Read existing log\n",
        "            df = pd.read_csv(log_path)\n",
        "            # Concatenate new record\n",
        "            df = pd.concat([df, record], ignore_index=True)\n",
        "        else:\n",
        "            # Use new record as initial DataFrame\n",
        "            df = record\n",
        "\n",
        "        # Save updated log\n",
        "        df.to_csv(log_path, index=False)\n",
        "\n",
        "        print(\"\\nAttendance Log Updated Successfully!\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating attendance log: {e}\")\n",
        "        return None\n",
        "\n",
        "def attendance_workflow(test_image_path, threshold=0.6):\n",
        "    \"\"\"Comprehensive attendance workflow\"\"\"\n",
        "    try:\n",
        "        # Mark attendance\n",
        "        attendance = mark_attendance(test_image_path, player_embeddings, threshold)\n",
        "\n",
        "        # Update attendance log\n",
        "        log = update_attendance_log(attendance)\n",
        "\n",
        "        return attendance, log\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in attendance workflow: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Diagnostic Information\n",
        "print(\"\\nAttendance Tracking System is Ready!\")\n",
        "print(\"Players in System:\", list(player_embeddings.keys()))\n",
        "print(\"\\nTo mark attendance:\")\n",
        "print(\"1. Prepare a test image with players\")\n",
        "print(\"2. Use attendance_workflow(image_path)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsmaQ8YLNAYr",
        "outputId": "5b18a3f5-121f-45e9-f994-6798f43f6a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attendance Results:\n",
            "devala: Absent ✗\n",
            "dimaria: Absent ✗\n",
            "matiaz: Absent ✗\n",
            "messi: Absent ✗\n",
            "neimar: Present ✓\n",
            "ronalu: Absent ✗\n",
            "\n",
            "Attendance Log Updated Successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-120-d2fa666a54a3>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(face_embedding).unsqueeze(0)\n"
          ]
        }
      ],
      "source": [
        "# Replace with your actual image path\n",
        "test_image_path = '/content/neymar.jpg'\n",
        "attendance, log = attendance_workflow(test_image_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8-jJg33zWEzA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0edec934e7cc4c4a91477e434f7395ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c5274f98c304612b2fbfb0f5fab49ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56eda4820092482089ed408bcf58224b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d33a9467f6d7406e9ed23c092cf6ba9d",
              "IPY_MODEL_d692f541fe624c9a956251c370a9ef05",
              "IPY_MODEL_605f4acc75494a029ad3403740b2b4c7"
            ],
            "layout": "IPY_MODEL_9b58b41631c44f2a98b76d72b3804600"
          }
        },
        "605f4acc75494a029ad3403740b2b4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f00e4a604f40318dd8913235a3088f",
            "placeholder": "​",
            "style": "IPY_MODEL_3c5274f98c304612b2fbfb0f5fab49ff",
            "value": " 107M/107M [00:01&lt;00:00, 120MB/s]"
          }
        },
        "8020f5630e7d48d48b9f5066b5b267a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f00e4a604f40318dd8913235a3088f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b58b41631c44f2a98b76d72b3804600": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd51a6eaa3304ffd9045fba69436a2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d33a9467f6d7406e9ed23c092cf6ba9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0edec934e7cc4c4a91477e434f7395ab",
            "placeholder": "​",
            "style": "IPY_MODEL_d8279d07e6964ee48d497b1d60649963",
            "value": "100%"
          }
        },
        "d692f541fe624c9a956251c370a9ef05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8020f5630e7d48d48b9f5066b5b267a8",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd51a6eaa3304ffd9045fba69436a2a6",
            "value": 111898327
          }
        },
        "d8279d07e6964ee48d497b1d60649963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
